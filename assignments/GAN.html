<!DOCTYPE html>
<html lang="zh-TW">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>GAN & VAE Implementation | Project Portfolio</title>

    <link rel="stylesheet" href="../css/bootstrap.css">
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="../css/components.css">
    
    <link rel="stylesheet" href="../css/fontawesome-all.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        .code-block {
            background-color: #f8f9fa;
            border-left: 4px solid #007bff;
            padding: 15px;
            margin: 20px 0;
            overflow-x: auto;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.9rem;
        }
        .math-section {
            background-color: #fff;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            margin-bottom: 20px;
        }
        .highlight-text {
            color: #d63384;
            font-weight: bold;
        }
    </style>
</head>

<body>
    <div class="wrapper">
        <nav id="sidebar">
            <div class="sidebar-header">
                <h3>Portfolio</h3>
            </div>

            <ul class="list-unstyled components">
                <li><a href="../index.html">Home</a></li>
                <li class="active">
                    <a href="#pageSubmenu" data-toggle="collapse" aria-expanded="true" class="dropdown-toggle">Assignments</a>
                    <ul class="collapse show list-unstyled" id="pageSubmenu">
                        <li><a href="DL.html">Deep Learning</a></li>
                        <li><a href="RL1.html">Reinforcement Learning 1</a></li>
                        <li><a href="RL2.html">Reinforcement Learning 2</a></li>
                        <li><a href="YOLO.html">YOLO Object Detection</a></li>
                        <li class="active"><a href="GAN_VAE.html">GAN & VAE</a></li>
                    </ul>
                </li>
            </ul>
        </nav>

        <div id="content">
            <nav class="navbar navbar-expand-lg navbar-light bg-light">
                <div class="container-fluid">
                    <button type="button" id="sidebarCollapse" class="btn btn-info">
                        <i class="fas fa-align-left"></i>
                        <span>Toggle Sidebar</span>
                    </button>
                    <h2 class="ml-3">Generative Models: GAN & VAE</h2>
                </div>
            </nav>

            <div class="container-fluid">
                
                <div class="row">
                    <div class="col-md-12">
                        <div class="card mb-4">
                            <div class="card-body">
                                <h3 class="card-title">Overview</h3>
                                <p class="card-text">
                                    本頁面記錄了生成對抗網路 (GAN) 與變分自編碼器 (VAE) 的實作細節，包含核心數學公式的推導以及 PyTorch 代碼的對應實現。重點在於解決 GAN 訓練不穩定問題 (Spectral Norm, Non-Saturating Loss) 以及 VAE 的 Reparameterization Trick。
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="row">
                    <div class="col-md-12">
                        <h3 class="mb-3">1. Generative Adversarial Networks (GAN)</h3>
                    </div>
                    
                    <div class="col-md-6">
                        <div class="math-section h-100">
                            <h4>數學原理：Minimax Game</h4>
                            <p>GAN 的訓練是一個零和博弈 (Zero-Sum Game)，原始目標函數如下：</p>
                            <p>$$ \min_G \max_D V(D, G) = \mathbb{E}_{x \sim p_{data}} [\log D(x)] + \mathbb{E}_{z \sim p_z} [\log(1 - D(G(z)))] $$</p>
                            <hr>
                            <h5>Non-Saturating Loss</h5>
                            <p>在訓練初期，Generator 生成效果差，Discriminator 很容易區分真假，導致 $\log(1 - D(G(z)))$ 的梯度趨近於 0 (Gradient Vanishing)。</p>
                            <p>解決方案是修改 Generator 的目標函數：</p>
                            <p class="text-center">從 <strong>最小化</strong> $\mathbb{E}[\log(1 - D(G(z)))]$</p>
                            <p class="text-center">改為 <strong>最大化</strong> $\mathbb{E}[\log(D(G(z)))]$</p>
                            <p>這在實作上對應到將 Fake Data 的 Label 設為 1 來計算 BCELoss。</p>
                        </div>
                    </div>

                    <div class="col-md-6">
                        <div class="math-section h-100">
                            <h4>實作重點：Discriminator & Stability</h4>
                            <p>為了穩定訓練，在 Discriminator 中使用了 <span class="highlight-text">Spectral Normalization</span> 以滿足 Lipschitz 連續性。</p>
                            <p>同時回傳 <code>Logit</code> (用於 Loss 計算，數值更穩定) 與 <code>Prob</code> (用於觀測)。</p>
<pre class="code-block"><code>class Discriminator(nn.Module):
    def __init__(self, ch=64):
        super(Discriminator, self).__init__()
        # 使用 Spectral Norm 防止梯度爆炸
        self.conv1 = Conv2d(3, ch, 5, stride=2)
        self.conv2 = nn.utils.spectral_norm(Conv2d(ch, ch*2, 5, stride=2))
        self.conv3 = nn.utils.spectral_norm(Conv2d(ch*2, ch*4, 5, stride=2))
        self.conv4 = nn.utils.spectral_norm(Conv2d(ch*4, ch*8, 5, stride=2))
        self.fc = nn.utils.spectral_norm(nn.Linear(ch*8*4*4, 1))

    def forward(self, x):
        # ... (Conv Layers with LeakyReLU) ...
        x = torch.flatten(x, 1)
        d_logit = self.fc(x)        # Range: (-inf, inf)
        d_prob = torch.sigmoid(d_logit) # Range: (0, 1)
        return d_prob, d_logit</code></pre>
                        </div>
                    </div>

                    <div class="col-md-12 mt-3">
                        <div class="math-section">
                            <h4>Training Loop Strategy</h4>
                            <p>在更新 Discriminator 時，必須使用 <code>detach()</code> 切斷傳向 Generator 的梯度。</p>
<pre class="code-block"><code># 1. Update Discriminator
# Real Data
d_real, _ = netDis(real_inputs)
d_real_loss = nn.BCELoss()(d_real, torch.ones(...)) # Label = 1

# Fake Data (Detach G)
fake_inputs = netGen(z)
d_fake, _ = netDis(fake_inputs.detach()) # 關鍵：Detach
d_fake_loss = nn.BCELoss()(d_fake, torch.zeros(...)) # Label = 0

# 2. Update Generator
# Generator 想騙過 D，所以 Label 設為 1
d_fake_for_g, _ = netDis(fake_inputs)
g_loss = nn.BCELoss()(d_fake_for_g, torch.ones(...)) # Non-Saturating trick</code></pre>
                        </div>
                    </div>
                </div>

                <hr class="my-5">

                <div class="row">
                    <div class="col-md-12">
                        <h3 class="mb-3">2. Variational Autoencoders (VAE)</h3>
                    </div>

                    <div class="col-md-6">
                        <div class="math-section h-100">
                            <h4>數學原理：ELBO & Reparameterization</h4>
                            <p>VAE 的目標是最大化 Evidence Lower Bound (ELBO)：</p>
                            <p>$$ \text{ELBO} = \mathbb{E}_{q}[\log p(x|z)] - D_{KL}(q(z|x) \parallel p(z)) $$</p>
                            <ul>
                                <li><strong>第一項 (Reconstruction Loss)：</strong> 希望重建圖像與原圖越像越好 (MSE)。</li>
                                <li><strong>第二項 (KL Divergence)：</strong> 希望 Latent Space $z$ 的分佈接近標準常態分佈。</li>
                            </ul>
                            <hr>
                            <h5>Reparameterization Trick</h5>
                            <p>由於直接從 $N(\mu, \sigma^2)$ 採樣不可微，我們將隨機性轉移到 $\epsilon$：</p>
                            <p>$$ z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim N(0, I) $$</p>
                        </div>
                    </div>

                    <div class="col-md-6">
                        <div class="math-section h-100">
                            <h4>實作重點：Forward & Loss</h4>
                            <p>實作中，我們預測 <code>logvar</code> 而不是 $\sigma$，以確保數值穩定性。</p>
<pre class="code-block"><code># Forward Pass (Reparameterization)
z_mean, z_logvar = netEnc(data)
std = torch.exp(0.5 * z_logvar) # sigma = exp(0.5 * log_var)
eps = torch.randn_like(std)     # Sample epsilon
z = z_mean + eps * std          # Reparameterize
x_recon = netDec(z)

# Loss Computation
# 1. Reconstruction Loss
rec_loss = nn.MSELoss()(x_recon, data)

# 2. KL Divergence (Analytic solution)
kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())

# Total Loss (beta-VAE: beta=0.0001 to prevent collapse)
loss = rec_loss + 0.0001 * kl_loss</code></pre>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>

    <script src="../js/jquery.min.js"></script>
    <script src="../js/bootstrap.min.js"></script>
    <script src="../js/animation.js"></script>
    <script>
        $(document).ready(function () {
            $('#sidebarCollapse').on('click', function () {
                $('#sidebar').toggleClass('active');
            });
        });
    </script>
</body>

</html>