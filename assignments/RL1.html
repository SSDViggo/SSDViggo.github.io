<!DOCTYPE html>
<html lang="zh-Hant">
<head>
  <meta charset="UTF-8">
  <title>作業名稱 | 江宇綸</title>
  
  <link rel="stylesheet" href="../css/styles.css">
  
  <script>
    window.addEventListener('load', () => {
      document.body.classList.add('loaded');
    });
  </script>
</head>

<body class="fade-body">
  
  <div class="top-buttons">
    <a href="../index.html" class="btn">回首頁</a>
  </div>

  <div class="container project-detail">
    
    <a href="../index.html" class="btn">← Back to Home</a>
    
    <h1>RL lab1</h1>
    
    <section>
      <h2>作業簡介</h2>
      <p>
        用DDQN來訓練RL模型，遊玩Atari的Pong，最後做到能穩定勝利。
      </p>
    </section>

    <section>
      <h2>使用技術 / 環境</h2>
      <p>
        <ul>
          <li>Language:  Python 3.9</li>
          <li>Framework: PyTorch</li>
          <li>Dataset:   ALE/Pong-v5</li>
          <li>Alogrithm: DDQN</li>
        </ul>
      </p>
    </section>

    <section>
      <h2>方法介紹 (Methodology)</h2>
      
      <div class="method-block">
        <h3>1. 狀態預處理 (State Preprocessing)</h3>
        <p>
          由於 Atari 原始畫面為 210x160 RGB 圖像，直接輸入模型會導致運算量過大。因此我進行了以下處理：
        </p>
        <ul>
          <li><strong>Grayscale：</strong> 對每個像素的RGB相加取平均，
            再套上threshold讓圖片變成黑白的。</li>
                <div class="code-box">
<pre><code>state = state.astype(np.float32).mean(2) / 255.0
state[state > th] = 1.0
state[state <= th] = 0.0</code></pre>
</div>
          <li><strong>Resizing：</strong> 將圖片壓縮至 84x84 像素。</li>
          <li><strong>Frame Stacking：</strong> 堆疊連續 4 幀畫面，讓模型能感知球的「速度」與「方向」。</li>
        </ul>
      </div>

      <div class="method-block">
        <h3>2. 模型架構 (QNet Architecture)</h3>
        <img src="../images/RL1/model_architecture.png" alt="RL model_architecture" style="max-width: 100%; border-radius: 10px;">
        <p>
          使用三層卷積層 (Convolutional Layers) 提取特徵，後接全連接層 (Fully Connected) 輸出動作價值 (Q-value)。
        </p>
        
      </div>
      <div class="method-block">
        <h3>3. 演算法簡介 (Q-learning)</h3>
        <img src="../images/RL1/DQN.webp" alt="Q-learning" style="max-width: 100%; border-radius: 10px;">
        <p>
          經過上面的方式找到的新參數一定會比較好(證明略)，其中Q值是指給定某個state下，最好的action所能得到的total_reward，以前的方法是建表紀錄並更新，如今採用NN來學習並預測。
        </p>
      </div>
      
      <div class="method-block">
        <h3>4. 演算法簡介 (DDQN)</h3>
        <img src="../images/RL1/Q-learning.png" alt="Q-learning" style="max-width: 100%; border-radius: 10px;">
        <p>
          DQN由於預測下一刻Q值得NN和選擇此刻動作的NN是同一個，常常會高估結果，導致收斂不問定，為了提升收斂穩定性，本專案採用了 <strong>Double DQN</strong>，透過分離「選擇動作」與「計算目標價值」的網路，解決過度估計 (Overestimation) 的問題。
        </p>
        <p>
            DQN和DDQN的差異如下:
        </p>
        <div class="code-comparison">
          
          <div class="code-column">
            <h4>DQN Implementation</h4>
            <div class="code-box">
<pre><code># DQN: 直接取 Target Net 的最大 Q 值
# 缺點：容易過度估計 (Overestimation)

next_q_values = target_net(next_states)
max_next_q = next_q_values.max(1)[0]

expected_q = rewards + gamma * max_next_q</code></pre>
            </div>
          </div>

          <div class="code-column">
            <h4>Double DQN Implementation</h4>
            <div class="code-box">
<pre><code># DDQN: 解耦 (Decoupling)
# 1. 用 Local Net 選動作 (Argmax)
# 2. 用 Target Net 算該動作的 Q 值

action_idx = eval_net(next_states).argmax(1)
next_q_values = target_net(next_states)
# gather: 根據 action_idx 取出對應的 Q 值
next_q = next_q_values.gather(1, action_idx)
expected_q = rewards + gamma * next_q</code></pre>
            </div>
          </div>
        </div>
       
      </div> 

    </section>

    <section>
      <h2>常用技巧</h2>
       <ul>
          <li><strong>Replay Buffer：</strong> 儲存過去 10,000 筆經驗，隨機採樣進行訓練，打破資料相關性。</li>
          <li><strong>Epsilon-Greedy：</strong> 訓練初期保持高探索率，隨著訓練步數增加線性衰減 Epsilon 值。</li>
        </ul>
    </section>

    <section>
      <h2>實作成果</h2>
      <p>
        下方是模型訓練過程的 Loss 變化圖，以及最終的辨識結果。
      </p>

      <div style="margin-top: 20px;">
        <h3>Training Loss</h3>
        <p>
        <strong>Step: 716 | Reward: 1.000 / 11.000</strong>
        </p>
      </div>

      <div style="margin-top: 20px;">
        <h3>Demo 結果</h3>
        <img src="../images/RL1/result.png" 
             data-gif="../images/RL1/eval.gif" 
             class="hover-gif" 
             alt="Demo截圖" 
             style="max-width: 100%; border-radius: 10px; cursor: pointer;">
      </div>
    </section>

    <section>
      <h2>相關資源</h2>
      <p style="margin-top: 0;">點擊下方按鈕查看完整程式碼或文件。</p>
      <a href="https://github.com/SSDViggo/ML_code" target="_blank" class="btn">View on GitHub</a>
    </section> 
  </div>
    
  <script>
    // 原有的淡入動畫設定
    window.addEventListener('load', () => {
      document.body.classList.add('loaded');
    });

    // ========= 新增：GIF 滑鼠懸停播放功能 =========
    document.addEventListener("DOMContentLoaded", () => {
      // 找到所有帶有 hover-gif class 的圖片
      const gifs = document.querySelectorAll('.hover-gif');

      gifs.forEach(img => {
        // 1. 先把原本的靜態圖路徑記下來
        const staticSrc = img.src;
        // 2. 取得 GIF 的路徑
        const gifSrc = img.getAttribute('data-gif');

        // 預先載入 GIF，避免滑鼠移上去時閃爍或等待
        const preloadImage = new Image();
        preloadImage.src = gifSrc;

        // 3. 滑鼠移入：換成 GIF
        img.addEventListener('mouseenter', () => {
          img.src = gifSrc;
        });

        // 4. 滑鼠移出：換回靜態圖
        img.addEventListener('mouseleave', () => {
          img.src = staticSrc;
        });
      });
    });
  </script>
</body>
</html>