<!DOCTYPE html>
<html lang="zh-Hant">
<head>
  <meta charset="UTF-8">
  <title>RL Lab1 | DDQN Pong</title>
  <link rel="stylesheet" href="../css/styles.css">
  <script>
    window.addEventListener('load', () => {
      document.body.classList.add('loaded');
    });
  </script>
</head>

<body class="fade-body">
  <div class="top-buttons">
    <a href="../index.html" class="btn">回首頁</a>
  </div>

  <div class="container project-detail">
    <a href="../index.html" class="btn">← Back to Home</a>
    
    <h1>Reinforcement Learning Lab 1</h1>
    <p class="subtitle">Deep Q-Network (DQN) & Double DQN 實作</p>

    <hr>

    <section>
      <h2>作業簡介</h2>
      <p>本專案實作 <strong>Double DQN (DDQN)</strong> 模型來訓練 AI 遊玩 Atari 經典遊戲 <code>Pong-v5</code>。目標是讓 Agent 從原始像素畫面中學習，最終達到能穩定擊敗電腦的水平。</p>
    </section>

    <section>
      <h2>使用技術 / 環境</h2>
      <div class="tech-stack">
        <ul>
          <li><strong>Language:</strong> Python 3.9</li>
          <li><strong>Framework:</strong> PyTorch</li>
          <li><strong>Environment:</strong> Gymnasium (ALE/Pong-v5)</li>
          <li><strong>Algorithm:</strong> Double Deep Q-Network (DDQN)</li>
        </ul>
      </div>
    </section>

    <section>
      <h2>方法介紹 (Methodology)</h2>
      
      <div class="method-block">
        <h3>1. 狀態預處理 (State Preprocessing)</h3>
        <p>Atari 原始畫面（210x160 RGB）過於龐大。為了加速訓練，我進行了以下預處理：</p>
        <ul>
          <li><strong>Binarization：</strong> 將圖片轉為灰階後進行二值化處理（Thresholding），簡化特徵。</li>
          <li><strong>Resizing：</strong> 壓縮至 84x84 像素以減少運算負擔。</li>
          <li><strong>Frame Stacking：</strong> 堆疊連續 4 幀畫面，賦予模型感知「速度」與「方向」的時間維度資訊。</li>
        </ul>
        <div class="code-box">
<pre><code># 像素處理示範
state = state.astype(np.float32).mean(2) / 255.0
state[state > th] = 1.0  # 二值化
state[state <= th] = 0.0</code></pre>
        </div>
      </div>

      <div class="method-block">
        <h3>2. 模型架構 (Q-Network Architecture)</h3>
        <p>採用卷積神經網路 (CNN) 作為特徵提取器，後接全連接層輸出各個動作的 <strong>Q-value</strong>。</p>
        <img src="../images/RL1/model_architecture.png" alt="CNN Architecture" class="method-img">
      </div>
      
      <div class="method-block">
        <h3>3. 從 DQN 到 Double DQN</h3>
        <p>傳統 DQN 常因同時使用同一組參數「選擇動作」與「評估價值」，導致嚴重的<strong>過度估計 (Overestimation)</strong> 問題。DDQN 透過分離這兩個過程來提升穩定性：</p>
        
        <div class="code-comparison">
          <div class="code-column">
            <h4>DQN (容易高估)</h4>
            <div class="code-box">
<pre><code># 直接取 Target Net 的最大值
max_next_q = target_net(next_s).max(1)[0]
expected_q = r + gamma * max_next_q</code></pre>
            </div>
          </div>

          <div class="code-column">
            <h4>Double DQN (解耦優化)</h4>
            <div class="code-box">
<pre><code># 用 Eval Net 選動作，Target Net 算價值
best_a = eval_net(next_s).argmax(1)
next_q = target_net(next_s).gather(1, best_a)
expected_q = r + gamma * next_q</code></pre>
            </div>
          </div>
        </div>
      </div> 
    </section>

    <section>
      <h2>訓練技巧 (Training Tricks)</h2>
      <div class="trick-grid">
        <div class="trick-item">
          <h4>Experience Replay</h4>
          <p>使用 10,000 大小的 Replay Buffer，隨機採樣以打破資料間的相關性 (Temporal Correlation)。</p>
        </div>
        <div class="trick-item">
          <h4>Epsilon-Greedy</h4>
          <p>訓練初期維持高探索率 ($\epsilon$)，隨步數增加進行線性衰減，在「探索」與「利用」間取得平衡。</p>
        </div>
      </div>
    </section>

    <hr>

    <section>
      <h2>實作成果</h2>
      <div class="result-container">
        <div class="chart-block">
          <h3>Training Progress</h3>
          <div class="stats-box">
            <p><strong>Training Step:</strong> 716 / Episode</p>
            <p><strong>Final Reward:</strong> +11.0 (Stable Win)</p>
          </div>
          <p>經過長時間訓練後，Agent 已能精確預測球路，並透過快速移動球拍封鎖電腦的所有進攻。Loss 曲線顯示在 100 萬步後趨於穩定。</p>
        </div>

        <div style="margin-top: 20px;">
        <h3>Demo 結果</h3>
        <img src="../images/RL1/result.png" 
             data-gif="../images/RL1/eval.gif" 
             class="hover-gif" 
             alt="Demo截圖" 
             style="max-width: 100%; border-radius: 10px; cursor: pointer;">
      </div>
      </div>
    </section>

    <section class="resources">
      <h2>相關資源</h2>
      <p>點擊下方按鈕查看完整的完整程式碼實作。</p>
      <a href="https://github.com/SSDViggo/ML_code" target="_blank" class="btn btn-github">View on GitHub</a>
    </section> 
  </div>

  <footer>
    <p>&copy; 2026 江宇綸 | RL Lab Report</p>
  </footer>

</body>
</html>
