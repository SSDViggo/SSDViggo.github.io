<!DOCTYPE html>
<html lang="zh-Hant">
<head>
  <meta charset="UTF-8">
  <title>RL Lab2 | PPO BipedalWalker-v3</title>
  <link rel="stylesheet" href="../css/styles.css">
  <script>
    window.addEventListener('load', () => {
      document.body.classList.add('loaded');
    });
  </script>
</head>

<body class="fade-body">
  <div class="top-buttons">
    <a href="../index.html" class="btn">回首頁</a>
  </div>

  <div class="container project-detail">
    <a href="../index.html" class="btn">← Back to Home</a>
    
    <h1>Reinforcement Learning Lab 2</h1>
    <p class="subtitle">Proximal Policy Optimization (PPO) 實作</p>

    <hr>

    <section>
      <h2>作業簡介</h2>
      <p>實作 PPO 演算法並訓練 Agent 在 <code>BipedalWalker-v3</code> 環境中穩定行走。透過 Actor-Critic 架構，使模型能夠在連續動作空間中學習最佳步態並順利到達終點。</p>
    </section>

    <section>
      <h2>使用技術 / 環境</h2>
      <div class="tech-stack">
        <ul>
          <li><strong>Language:</strong> Python 3.9</li>
          <li><strong>Framework:</strong> PyTorch</li>
          <li><strong>Environment:</strong> Gymnasium (BipedalWalker-v3)</li>
          <li><strong>Algorithm:</strong> PPO (Proximal Policy Optimization)</li>
        </ul>
      </div>
    </section>

    <section>
      <h2>方法介紹 (Methodology)</h2>
      
      <div class="method-block">
        <h3>1. 狀態預處理 (State Preprocessing)</h3>
        <p>環境輸入為 24 維向量（如關節角度、雷達數據）。由於原始數值差異過大，使用 <code>NormalizeObservation</code> 進行標準化，並透過 <code>np.clip</code> 限制範圍，確保訓練穩定性。</p>
        <div class="code-box">
<pre><code>env = gym.make('BipedalWalker-v3')
env = NormalizeObservation(env)
env = TransformObservation(env, lambda obs: np.clip(obs, -10, 10))</code></pre>
        </div>
      </div>

      <div class="method-block">
        <h3>2. 模型架構 (Actor-Critic Architecture)</h3>
        <p>採用 Actor-Critic 結構，將策略與價值評估解耦：</p>
        <ul>
          <li><strong>PolicyNet (Actor)：</strong> 輸出高斯分佈參數（Mean & Std），用於在連續空間採樣動作。</li>
          <li><strong>ValueNet (Critic)：</strong> 預測狀態價值（$V$ 值），作為計算 Advantage 的基準。</li>
        </ul>
        <img src="../images/RL2/model.png" alt="model architecture" class="method-img">
      </div>
        
      <div class="method-block">
        <h3>3. 訓練細節</h3>
        <p><strong>資料蒐集：</strong> PPO 屬 On-policy 演算法，先讓策略與環境互動蒐集資料（Trajectory），再利用 Importance Sampling 技巧進行多次迭代更新。</p>
        <p><strong>損失函數與 GAE：</strong> 為了穩定更新，計算總損失時包含了 Policy Loss（Clipped）、Value Loss 以及 Entropy Bonus（鼓勵探索）。</p>
        <img src="../images/RL2/gae.png" alt="gae formula" class="method-img">
        <p>使用 <strong>GAE (Generalized Advantage Estimation)</strong> 平衡偏差與變異，並透過 <code>ratio</code> 計算新舊策略的差異：</p>
        <div class="code-box">
<pre><code>ratio = (new_log_probs - old_log_probs).exp()</code></pre>
        </div>
      </div>
    </section>

    <hr>

    <section>
      <h2>實作成果</h2>
      <div class="result-container">
        <div class="chart-block">
          <h3>Training Statistics</h3>
          <img src="../images/RL2/loss.png" alt="Training Loss" class="result-img">
          <p class="caption">
            平均 Reward 穩定上升且 Policy Loss 下降，顯示模型正確收斂。由於 Episode 長度變動大，Value 預測難度較高導致 Value Loss 波動，且 Entropy 權重固定使得模型在後期仍保持高探索性。
          </p>
          <div class="stats-box">
            <p><strong>Evaluation Total Reward:</strong> 22.13 (Length: 365)</p>
            <p><strong>Final 100-Avg Reward:</strong> 271.10</p>
          </div>
        </div>

        <div class="demo-block">
          <h3>Demo 成果 (Video)</h3>
          <video width="100%" controls autoplay loop muted class="demo-video">
            <source src="../images/RL2/eval_epoch_test_result-episode-0.mp4" type="video/mp4">
            您的瀏覽器不支持影片播放。
          </video>
        </div>
      </div>
    </section>

    <section class="resources">
      <h2>相關資源</h2>
      <p>完整程式碼與訓練細節請參考 GitHub 儲存庫。</p>
      <a href="https://github.com/SSDViggo/ML_code" target="_blank" class="btn btn-github">View on GitHub</a>
    </section> 
  </div>

  <footer>
    <p>&copy; 2026 江宇綸 | RL Lab Report</p>
  </footer>
</body>
</html>